¡NOTE! Python 3.7+ required in order to use this app and its associated scripts

1. Make sure you have your model image folders in the 'images' directory
	a) Each "Model" is represented by a folder of images that were generated by a given GenAI checkpoint (cf. Stable Diffusion)
	b) Ideally, each "Model" corresponds to controlled changes in either, i. model training hyperparameters, or ii. image generation hyperparameters
	c) The "Models" will be compared using two modes:
       		i. Two-Model Direct Comparison (User Defined pair on GUI; e.g. default 1,2 compares the first and second model folders (ordered by name, ascending)), AND 
     		ii. Multi-Model Round-Robin tournament style compares every model in a pair-wise way to generate a mean score over all possible pairings
	  
2. Place a fixed set of reference images in the 'references' directory
   in order to use the CREATE face-distance script
	a) Each "Model" will be compared to each reference image, where results for each model are collected together for all references
	b) Use images of the same subject, ideally, whose likeness you are trying to replicate 
	c) The more same-subject reference images you use (e.g. various poses, facial expressions, etc.) the better the resulting statistics will be

3. Put some, or all, of the example data directories in 'output', OR
   first run the CREATE script with `your` images, to test OR to use the BULK analysis script
	a) Distributions are created using the data generated by the CREATE script and put in the 'output' folder
	b) The two models being directly compared will have their distributions shown on a histogram and a Q-Q plot
		i. The center of the histogram gives a rough "eyeball" idea of the rough accuracy score (lower IS better) and the spread an idea of the model's flexibility (higher spread = more flexible)
		ii. How close a model's Q-Q plot is to a straight line shows how Gaussian (or normal) the distribution is;
	      perfectly random, uncorrelated comparisons should yield a normal distribution (i.e. your models are of random people, not the reference)
	c) Primary results will be displayed at the bottom of the GUI text box
		i. The full display holds all the individual Weight-Based scores
        	ii. The direct compare winner is in the 'Better Model' section
        	iii. Overall winner of the tournament is displayed last
		iv. Remember, lower IS better!

4. After running the BULK script, the figures can be pulled up by using the 'Open Last Saved Figure' button on the GUI
	(or found in '_EXTRAS' folder, along with extra BATS for debugging/troubleshooting the scripts when error messages aren't shown otherwise)
	a) Changes to the 'Models to Compare' require the BULK script to be re-run in order to update the figure!

5. Details shown in the GUI text box can also be found in the 'LOGS' directory
	a) The 'process_log' shows the progress of the CREATE script as it processes your images
	b) The 'output_stats' holds all the accuracy score results from the BULK script
	c) The 'metric_weight_normal_stats' is not normally shown in the GUI, but contains more detailed information on calculated metrics, weights, and the normality tests
	
6. [EXAMPLE] Check out the example data's folder for a short explanation of, and data from, some simple experiments performed using the early version of the scripts in the 'PLEASE_READ.txt' 

>>> ¡NOTE! Using the install_MGAS bat will automatically install the 'shape_predictor_5_face_landmarks.dat' and 'dlib_face_recognition_resnet_model_v1.dat' models (~ 30 MB), 
	needed for the DLib facial recognition and distance embeddings, into the correct 'DLIB' directory. Otherwise, these files are very easy to find online, e.g. on HuggingFace.co, or you can find the direct HF links if you open the install BAT with your text editor of choice.

>>> ¡NOTE! The correct directories are automatically set by the scripts, but you are able to change them using the GUI (these changes do not save after closing!)

>>> ¡NOTE! You can change the number of processes, i.e. CPU cores/threads, to use for the CREATE script, but be careful not to set it too high (< 10, the default 2 should work well enough for most, 4-6 for mid-range, 8-10 high-mid (10 on my i5-14600K), 11+ beast tier CPU)