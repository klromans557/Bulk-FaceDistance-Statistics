import os
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.stats import skew, kurtosis, shapiro, kstest, norm
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import logging
from io import StringIO

# Disable interactive mode for plotting
plt.ioff()  # Disable interactive mode

# Example GUI class implementation for demonstration purposes
class GUI:
    def display(self, content):
        # Implement the method to display content in your GUI
        print(content)  # For example purposes, it just prints the content

# Initialize the GUI object
gui = GUI()

########### BULK FACE-DISTANCE STATISTICS #################
# A script developed by klromans557 (a.k.a reaper557) with
# the help of GPT-4o. Thanks Zedd! (>'.')>[<3] [7/25/2024]
#
# The purpose of this script is to perform a statistical
# analysis of the data generated by the create_facedist 
# script. 
# A attempt is made at developing an "objective"
# score, called the General Accuracy Score (GAS)
# in which to test similar models for their ability to 
# replicate the likeness of the commonly trained subject.
# In other words, I was getting tired of "eyeballing" model
# output images and trying to figure out which one was doing
# a better job, and I wanted to quantify and automate this
# process. I also wanted to get a better grasp on how
# changes to training methodology, e.g. hyperparamter sett.,
# image quality in the training/REG data sets, and captions
# in the REG set (question that inspired me here) can affect
# a model's ability to replicate the subject's likeness.  
############################################################

# ===== USER DEFINED VARIABLES =============================
# Â¡NOTE! DO NOT CHANGE this one unless YOU know what you are doing <(o.o)>    
#order_of_import_of_metrics = ["Mean", "Median", "Range", "SD", "Skewness", "Excess Kurtosis", "IQR", "P90", "P10"]
order_of_import_of_metrics = ["Mean", "SD", "IQR", "Median", "Skewness", "Excess Kurtosis", "Range", "P90", "P10"] 
# ===== USER DEFINED VARIABLES =============================

# ===== FIXED GLOBALS ======================================
# Thresholds to switch between normality tests and
# for determining result from normality tests
sample_size_threshold = 5000 # S-W test cannot handle very large sample sizes like K-S can, but is quick otherwise
sw_threshold = 0.95 # "Shapiro-Wilk"
ks_threshold = 0.05 # "Kolmogorov-Smirnov"
pvalue_threshold = 0.05 # For testing the NULL-hypothesis

# Automatically sets directories
script_dir = os.path.dirname(os.path.abspath(__file__))
base_dir = os.path.join(script_dir, 'DIR', 'output')  # Directory containing model folders
logs_dir = os.path.join(script_dir, 'LOGS')

# Ensure the LOGS directory exists
if not os.path.exists(logs_dir):
    os.makedirs(logs_dir)

metric_log_file = os.path.join(logs_dir, 'metric_weight_normal_stats.txt')
output_stats_file = os.path.join(logs_dir, 'output_stats.txt')

# Check if the file exists
if not os.path.exists(metric_log_file):
    # Create an empty file
    with open(metric_log_file, 'w') as file:
        pass  # Just creating an empty file

if not os.path.exists(output_stats_file):
    # Create an empty file
    with open(output_stats_file, 'w') as file:
        pass  # Just creating an empty file

print(f"Metric log file path: {metric_log_file}")
print(f"Output stats file path: {output_stats_file}")
# ===== FIXED GLOBALS ======================================

# ===== LOGGING ============================================
log_stream = StringIO()  # Initialize log_stream here

# Configure logging for metrics and normality results
metrics_logger = logging.getLogger('metrics_logger')
metrics_logger.setLevel(logging.INFO)
metrics_handler = logging.FileHandler(metric_log_file, mode='w')
metrics_handler.setLevel(logging.INFO)
metrics_formatter = logging.Formatter('%(message)s')
metrics_handler.setFormatter(metrics_formatter)
metrics_logger.addHandler(metrics_handler)

# Configure logging for accuracy scores and final outcomes
main_logger = logging.getLogger('main_logger')
main_logger.setLevel(logging.INFO)
stream_handler = logging.StreamHandler(log_stream)
stream_handler.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(message)s')
stream_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)
main_logger.addHandler(stream_handler)
main_logger.addHandler(console_handler)

print("Logging configured.")
# ===== LOGGING ============================================

# ===== FUNCTION ZOO =======================================
def read_data_from_directory(directory):
    all_data = []
    try:
        for filename in os.listdir(directory):
            if filename.endswith(".txt"):
                filepath = os.path.join(directory, filename)
                with open(filepath, 'r') as file:
                    data = file.read().strip('[]').split(',')
                    cleaned_data = [float(value.strip()) for value in data if 0 <= float(value.strip()) <= 1]
                    all_data.extend(cleaned_data)
    except Exception as e:
        main_logger.error(f"An error occurred while reading {filepath}: {e}")
    return np.array(all_data)

def get_metric_values(metrics, order):
    metric_names = ["Mean", "Median", "Range", "SD", "Skewness", "Excess Kurtosis", "IQR", "P90", "P10"]
    metric_dict = dict(zip(metric_names, metrics))
    ordered_metrics = [metric_dict[name] for name in order]
    return ordered_metrics

def calculate_metrics(values):
    if len(values) == 0:
        return [np.nan] * len(order_of_import_of_metrics)
    mean = np.mean(values)
    med = np.median(values)
    data_range = np.max(values) - np.min(values)
    sd = np.std(values)
    skewness = skew(values)
    kurt = kurtosis(values)
    iqr = np.percentile(values, 75) - np.percentile(values, 25)
    perc_90 = np.percentile(values, 90)
    perc_10 = np.percentile(values, 10)

    metric_dict = {
        "Mean": mean,
        "Median": med,
        "Range": data_range,
        "SD": sd,
        "Skewness": skewness,
        "Excess Kurtosis": kurt,
        "IQR": iqr,
        "P90": perc_90,
        "P10": perc_10
    }

    ordered_metrics = [metric_dict[metric] for metric in order_of_import_of_metrics]
    return ordered_metrics

def round_and_format(value, decimals=4, threshold=1e-4):
    if abs(value) < threshold:
        return f"{value:.2e}"
    else:
        return round(value, decimals)

def compute_bin_size(numbers):
    bins = int(np.ceil(np.log2(len(numbers)) + 1))
    return bins

def create_histogram_and_display_metrics(numbers1, numbers2, label1, label2, metrics1, metrics2):
    ordered_metrics1 = get_metric_values(metrics1, order_of_import_of_metrics)
    ordered_metrics2 = get_metric_values(metrics2, order_of_import_of_metrics)
    
    label1_metrics = ', '.join([f'{name}: {value:.2f}' for name, value in zip(order_of_import_of_metrics, ordered_metrics1)])
    label2_metrics = ', '.join([f'{name}: {value:.2f}' for name, value in zip(order_of_import_of_metrics, ordered_metrics2)])
    
    fig, axs = plt.subplots(2, 1, figsize=(10, 10))  # Fixed figure size

    axs[0].hist(numbers1, bins=compute_bin_size(numbers1), alpha=0.5, 
                label=f'{label1} - {label1_metrics}', 
                edgecolor='black')
    axs[0].hist(numbers2, bins=compute_bin_size(numbers2), alpha=0.5, 
                label=f'{label2} - {label2_metrics}', 
                edgecolor='black')
    axs[0].set_xlabel('Value')
    axs[0].set_ylabel('Frequency')
    axs[0].legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=1)
    axs[0].grid(True)
    
    qq1 = stats.probplot(numbers1, dist="norm", plot=axs[1])
    line1 = axs[1].get_lines()[1]
    line1.set_color('green')
    line1.set_linestyle('--')
    scatter1 = axs[1].get_lines()[0]
    scatter1.set_color('blue')
    scatter1.set_markersize(scatter1.get_markersize() * 0.5)
    scatter1.set_label(f'{label1} Data')
    line1.set_label(f'{label1} Trendline')

    qq2 = stats.probplot(numbers2, dist="norm", plot=axs[1])
    line2 = axs[1].get_lines()[3]
    line2.set_color('red')
    line2.set_linestyle('--')
    scatter2 = axs[1].get_lines()[2]
    scatter2.set_color('orange')
    scatter2.set_markersize(scatter2.get_markersize() * 0.5)
    scatter2.set_label(f'{label2} Data')
    line2.set_label(f'{label2} Trendline')
    
    common_markersize = 20
    start_point1 = [line1.get_xdata()[0], line1.get_ydata()[0]]
    end_point1 = [line1.get_xdata()[-1], line1.get_ydata()[-1]]
    axs[1].plot(start_point1[0], start_point1[1], 'gx', markersize=common_markersize)
    axs[1].plot(end_point1[0], end_point1[1], 'gx', markersize=common_markersize)

    start_point2 = [line2.get_xdata()[0], line2.get_ydata()[0]]
    end_point2 = [line2.get_xdata()[-1], line2.get_ydata()[-1]]
    axs[1].plot(start_point2[0], start_point2[1], 'r+', markersize=common_markersize)
    axs[1].plot(end_point2[0], end_point2[1], 'r+', markersize=common_markersize)

    axs[1].set_title('')
    
    slope1 = (end_point1[1] - start_point1[1]) / (end_point1[0] - start_point1[0])
    slope2 = (end_point2[1] - start_point2[1]) / (end_point2[0] - start_point2[0])

    scale_factor = 0.2
    y_scale_start = (1 - scale_factor) * start_point1[1] if (start_point2[1] > start_point1[1]) else (1 - scale_factor) * start_point2[1]
    y_scale_end = (1 + scale_factor) * end_point2[1] if (end_point2[1] > end_point1[1]) else (1 + scale_factor) * end_point1[1]
    
    if y_scale_end > 1:
        y_scale_end = 1
    if y_scale_start < 0:
        yscale_start = 0

    bbox_props = dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white", alpha=0.7)
    axs[1].text(0.95, 0.1, f'{label1} Slope: {slope1:.3f}', horizontalalignment='right', verticalalignment='center', transform=axs[1].transAxes, color='green', bbox=bbox_props)
    axs[1].text(0.95, 0.05, f'{label2} Slope: {slope2:.3f}', horizontalalignment='right', verticalalignment='center', transform=axs[1].transAxes, color='red', bbox=bbox_props)

    axs[1].legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)
    axs[1].grid(True)
    axs[1].set_ylim(y_scale_start, y_scale_end)

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    
    plot_filename = os.path.join(os.getcwd(), '_EXTRAS', 'comparison_plot.png')
    plt.savefig(plot_filename)
    print(f"Comparison plot saved to: {plot_filename}")
    plt.close(fig)  # Close the figure to avoid displaying it


def calculate_pca_weights(metrics, num_components=None):
    scaler = StandardScaler()
    standardized_metrics = scaler.fit_transform(metrics)
    pca = PCA()
    pca.fit(standardized_metrics)
    explained_variance_ratios = pca.explained_variance_ratio_
    
    if num_components:
        cumulative_variance = np.cumsum(explained_variance_ratios)
        weights = cumulative_variance / np.sum(cumulative_variance)
    else:
        weights = explained_variance_ratios
    
    if len(weights) < metrics.shape[1]:
        weights = np.pad(weights, (0, metrics.shape[1] - len(weights)), 'constant')
    weights = weights / np.sum(weights)  
    return weights

def calculate_inverse_variance_weights(metrics):
    variances = np.var(metrics, axis=0)
    inverse_variances = 1 / variances
    weights = inverse_variances / np.sum(inverse_variances)
    return weights

def calculate_ahp_weights():
    comparison_matrix = np.array([
        [1, 2, 3, 4, 5, 6, 7, 8, 9],
        [1/2, 1, 2, 3, 4, 5, 6, 7, 8],
        [1/3, 1/2, 1, 2, 3, 4, 5, 6, 7],
        [1/4, 1/3, 1/2, 1, 2, 3, 4, 5, 6],
        [1/5, 1/4, 1/3, 1/2, 1, 2, 3, 4, 5],
        [1/6, 1/5, 1/4, 1/3, 1/2, 1, 2, 3, 4],
        [1/7, 1/6, 1/5, 1/4, 1/3, 1/2, 1, 2, 3],
        [1/8, 1/7, 1/6, 1/5, 1/4, 1/3, 1/2, 1, 2],
        [1/9, 1/8, 1/7, 1/6, 1/5, 1/4, 1/3, 1/2, 1]
    ])

    column_sums = np.sum(comparison_matrix, axis=0)
    normalized_matrix = comparison_matrix / column_sums

    weights = np.mean(normalized_matrix, axis=1)
    return weights

def generate_synthetic_normal_metrics(mean, sd, size):
    synthetic_normal = np.random.normal(mean, sd, size)
    return calculate_metrics(synthetic_normal)

def calculate_fractional_percent_difference(data_metrics, synthetic_metrics):
    return [abs((d - s) / s) * 100 for d, s in zip(data_metrics, synthetic_metrics)]

def insert_newlines(text, line_length):
    lines = []
    while len(text) > line_length:
        split_pos = text[:line_length].rfind(' ')
        if split_pos == -1:
            split_pos = line_length
        lines.append(text[:split_pos])
        text = text[split_pos:].strip()
    lines.append(text)
    return '\n'.join(lines)


def perform_normality_test(values):
    if len(values) <= sample_size_threshold:
        test_result = shapiro(values)
        test_name = "Shapiro-Wilk"
    else:
        norm_samples = norm.rvs(size=len(values))
        test_result = kstest(values, 'norm', args=(np.mean(values), np.std(values)))
        test_name = "Kolmogorov-Smirnov"
    return test_result, test_name

def interpret_test_results(test_result, test_name):
    if test_name == "Shapiro-Wilk":
        W = test_result.statistic
        p_value = test_result.pvalue
        if p_value > pvalue_threshold and W > sw_threshold:
            normal_status = "Normal Status: EXPECTED!"
        elif p_value <= pvalue_threshold and W < sw_threshold:
            normal_status = "Normal Status: NOT expected!"
        else:
            normal_status = "Normal Status: ???"
    elif test_name == "Kolmogorov-Smirnov":
        D = test_result.statistic
        p_value = test_result.pvalue
        if p_value > pvalue_threshold and D <= ks_threshold:
            normal_status = "Normal Status: EXPECTED!"
        elif p_value <= pvalue_threshold and D > ks_threshold:
            normal_status = "Normal Status: NOT expected!"
        else:
            normal_status = "Normal Status: ???"
    return normal_status

def perform_comparison(dir_model_1, dir_model_2):
    values_1 = read_data_from_directory(dir_model_1)
    values_2 = read_data_from_directory(dir_model_2)

    if values_1.size == 0 or values_2.size == 0:
        main_logger.warning(f"Skipping comparison between {dir_model_1} and {dir_model_2} due to empty data.")
        return [np.nan] * len(order_of_import_of_metrics), [np.nan] * len(order_of_import_of_metrics), [], [], [], [], []

    metrics_1 = calculate_metrics(values_1)
    metrics_2 = calculate_metrics(values_2)
    
    combined_metrics = np.array([metrics_1, metrics_2])

    uniform_weights = np.ones(combined_metrics.shape[1]) / combined_metrics.shape[1]
    subjective_weights = np.array([0.2, 0.2, 0.15, 0.15, 0.1, 0.1, 0.05, 0.025, 0.025])
    pca_weights = calculate_pca_weights(combined_metrics, num_components=combined_metrics.shape[1])
    inverse_variance_weights = calculate_inverse_variance_weights(combined_metrics)
    ahp_weights = calculate_ahp_weights()

    def calculate_scores(weights):
        general_accuracy_score_1 = np.dot(metrics_1, weights)
        general_accuracy_score_2 = np.dot(metrics_2, weights)
        return general_accuracy_score_1, general_accuracy_score_2

    uniform_score_1, uniform_score_2 = calculate_scores(uniform_weights)
    subjective_score_1, subjective_score_2 = calculate_scores(subjective_weights)
    pca_score_1, pca_score_2 = calculate_scores(pca_weights)
    inverse_variance_score_1, inverse_variance_score_2 = calculate_scores(inverse_variance_weights)
    ahp_score_1, ahp_score_2 = calculate_scores(ahp_weights)

    average_score_1 = np.mean([uniform_score_1, subjective_score_1, pca_score_1, inverse_variance_score_1, ahp_score_1])
    average_score_2 = np.mean([uniform_score_2, subjective_score_2, pca_score_2, inverse_variance_score_2, ahp_score_2])

    return average_score_1, average_score_2, uniform_weights, subjective_weights, pca_weights, inverse_variance_weights, ahp_weights


def round_robin_comparisons(model_dirs):
    num_models = len(model_dirs)
    comparison_results = {i: [] for i in range(num_models)}

    for i in range(num_models):
        for j in range(i + 1, num_models):
            agas_1, agas_2 = perform_comparison(model_dirs[i], model_dirs[j])[:2]
            agas_1 = np.array(agas_1)
            agas_2 = np.array(agas_2)
            
            if not np.isnan(agas_1).any():
                comparison_results[i].append(agas_1)
            if not np.isnan(agas_2).any():
                comparison_results[j].append(agas_2)

    return comparison_results

def extract_sections_for_gui(filename):
    sections_to_display = []
    display = False
    with open(filename, "r") as file:
        for line in file:
            if "Averaged General Accuracy Scores" in line:
                display = True
            elif "General Accuracy Scores - Uniform Weights" in line:
                display = False
            if display:
                sections_to_display.append(line)
    return ''.join(sections_to_display)

def is_directory_empty(directory):
    return not any(os.scandir(directory))

# ===== FUNCTION ZOO =======================================

def main():
    models_to_compare_directly = os.getenv('MODELS_TO_COMPARE', "1,2").split(',')
    
    try:
        models_to_compare_directly = [int(model.strip()) for model in models_to_compare_directly]
    except ValueError:
        print("Error: models_to_compare_directly must contain comma-separated integer values.")
        return
    
    if is_directory_empty(base_dir):
        print("Error: The 'output' folder is empty.")
        return
    
    model_dirs = [os.path.join(base_dir, subdir) for subdir in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, subdir))]
    
    if len(model_dirs) < 2:
        print("Error: There must be at least two model directories to compare.")
        return

    if len(models_to_compare_directly) != 2:
        print("Error: models_to_compare_directly must contain exactly two numbers.")
        return

    model_idx_1, model_idx_2 = models_to_compare_directly
    if model_idx_1 <= 0 or model_idx_2 <= 0:
        print("Error: Model indices must be positive integers.")
        return
    if model_idx_1 > len(model_dirs) or model_idx_2 > len(model_dirs):
        print(f"Error: Model indices must be less than or equal to the number of model directories ({len(model_dirs)}).")
        return

    model_idx_1 -= 1
    model_idx_2 -= 1

    values_1 = read_data_from_directory(model_dirs[model_idx_1])
    values_2 = read_data_from_directory(model_dirs[model_idx_2])

    if values_1.size == 0 or values_2.size == 0:
        print(f"Error: One or both specified model directories have no valid data.")
        return

    metrics_1 = calculate_metrics(values_1)
    metrics_2 = calculate_metrics(values_2)

    synthetic_metrics_1 = generate_synthetic_normal_metrics(metrics_1[0], metrics_1[3], len(values_1))
    synthetic_metrics_2 = generate_synthetic_normal_metrics(metrics_2[0], metrics_2[3], len(values_2))

    fractional_percent_diff_1 = calculate_fractional_percent_difference(metrics_1, synthetic_metrics_1)
    fractional_percent_diff_2 = calculate_fractional_percent_difference(metrics_2, synthetic_metrics_2)
    
    normality_test_1, test_name_1 = perform_normality_test(values_1)
    normality_test_2, test_name_2 = perform_normality_test(values_2)
    
    normal_status_1 = interpret_test_results(normality_test_1, test_name_1)
    normal_status_2 = interpret_test_results(normality_test_2, test_name_2)

    metrics_logger.info("="*50)
    metrics_logger.info("Metric Values")
    metrics_logger.info("="*50)

    metric_dict_1 = {name: round_and_format(value) for name, value in zip(order_of_import_of_metrics, get_metric_values(metrics_1, order_of_import_of_metrics))}
    metric_dict_2 = {name: round_and_format(value) for name, value in zip(order_of_import_of_metrics, get_metric_values(metrics_2, order_of_import_of_metrics))}

    metrics_logger.info(f"Metrics for Model {model_idx_1 + 1}: %s", metric_dict_1)
    metrics_logger.info(f"Metrics for Model {model_idx_2 + 1}: %s", metric_dict_2)
   
    metrics_logger.info("\n" + "="*50)
    metrics_logger.info("Weight Values")
    metrics_logger.info("="*50)
    
    average_score_1, average_score_2, uniform_weights, subjective_weights, pca_weights, inverse_variance_weights, ahp_weights = perform_comparison(model_dirs[model_idx_1], model_dirs[model_idx_2])
    
    metrics_logger.info("Uniform Weights: %s", [round_and_format(w) for w in uniform_weights])
    metrics_logger.info("Subjective Weights: %s", [round_and_format(w) for w in subjective_weights])
    metrics_logger.info("PCA-Based Weights: %s", [round_and_format(w) for w in pca_weights])
    metrics_logger.info("Inverse Variance-Based Weights: %s", [round_and_format(w) for w in inverse_variance_weights])
    metrics_logger.info("AHP-Based Weights: %s", [round_and_format(w) for w in ahp_weights])

    synthetic_metric_dict_1 = {name: round_and_format(value) for name, value in zip(order_of_import_of_metrics, get_metric_values(synthetic_metrics_1, order_of_import_of_metrics))}
    synthetic_metric_dict_2 = {name: round_and_format(value) for name, value in zip(order_of_import_of_metrics, get_metric_values(synthetic_metrics_2, order_of_import_of_metrics))}

    metrics_logger.info("\n" + "="*50)
    metrics_logger.info("Normal Tests")
    metrics_logger.info("="*50)

    metrics_logger.info(f"\nSynthetic Metrics for Model {model_idx_1 + 1}: %s", synthetic_metric_dict_1)
    metrics_logger.info(f"Synthetic Metrics for Model {model_idx_2 + 1}: %s", synthetic_metric_dict_2)

    metrics_logger.info(f"\nSynthetic Normal Distribution Metrics Comparison for Model {model_idx_1 + 1} (percent differences):")
    for name, diff in zip(order_of_import_of_metrics, fractional_percent_diff_1):
        metrics_logger.info(f"{name}: {round_and_format(diff):.2f}%")

    metrics_logger.info(f"\n{test_name_1} Test for Model {model_idx_1 + 1}: statistic={normality_test_1.statistic:.5f}, p-value={normality_test_1.pvalue:.5f}")
    metrics_logger.info(normal_status_1)

    metrics_logger.info(f"\nSynthetic Normal Distribution Metrics Comparison for Model {model_idx_2 + 1} (percent differences):")
    for name, diff in zip(order_of_import_of_metrics, fractional_percent_diff_2):
        metrics_logger.info(f"{name}: {round_and_format(diff):.2f}%")

    metrics_logger.info(f"\n{test_name_2} Test for Model {model_idx_2 + 1}: statistic={normality_test_2.statistic:.5f}, p-value={normality_test_2.pvalue:.5f}")
    metrics_logger.info(normal_status_2)

    metrics_logger.info("\n" + "="*50)
    
    with open(metric_log_file, "a") as f:
        f.write(log_stream.getvalue())
  
    log_stream.truncate(0)
    log_stream.seek(0)

    create_histogram_and_display_metrics(values_1, values_2, f'Model {model_idx_1 + 1}', f'Model {model_idx_2 + 1}', metrics_1, metrics_2)

    log_stream.truncate(0)
    log_stream.seek(0)

    def calculate_scores(weights):
        general_accuracy_score_1 = np.dot(metrics_1, weights)
        general_accuracy_score_2 = np.dot(metrics_2, weights)
        return general_accuracy_score_1, general_accuracy_score_2

    uniform_score_1, uniform_score_2 = calculate_scores(uniform_weights)
    subjective_score_1, subjective_score_2 = calculate_scores(subjective_weights)
    pca_score_1, pca_score_2 = calculate_scores(pca_weights)
    inverse_variance_score_1, inverse_variance_score_2 = calculate_scores(inverse_variance_weights)
    ahp_score_1, ahp_score_2 = calculate_scores(ahp_weights)

    wins_1 = 0
    wins_2 = 0

    if uniform_score_1 < uniform_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if subjective_score_1 < subjective_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if pca_score_1 < pca_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if inverse_variance_score_1 < inverse_variance_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    if ahp_score_1 < ahp_score_2:
        wins_1 += 1
    else:
        wins_2 += 1

    average_score_1 = np.mean([uniform_score_1, subjective_score_1, pca_score_1, inverse_variance_score_1, ahp_score_1])
    average_score_2 = np.mean([uniform_score_2, subjective_score_2, pca_score_2, inverse_variance_score_2, ahp_score_2])

    logger_common_symbol_length = 56
    new_line_char_lim = 56
    message1 = f"Model {model_idx_1 + 1} has the best chance replicating the reference!"
    message2 = f"Model {model_idx_2 + 1} has the best chance replicating the reference!"
    message3 = "Model determination is mixed, check 'Better Model' section."
    message4 = "No Model has a definitive outcome."
    formatted_message1 = insert_newlines(message1, new_line_char_lim)
    formatted_message2 = insert_newlines(message2, new_line_char_lim)
    formatted_message3 = insert_newlines(message3, new_line_char_lim)
    formatted_message4 = insert_newlines(message4, new_line_char_lim)

    symbol_separator1 = "-"
    symbol_separator2 = "="

    main_logger.info(symbol_separator1 * logger_common_symbol_length)
    main_logger.info("Accuracy Scores - Uniform Weights")
    main_logger.info(symbol_separator1 * logger_common_symbol_length)
    main_logger.info(f"Accuracy Score for Model {model_idx_1 + 1}: %.4f", round_and_format(uniform_score_1))
    main_logger.info(f"Accuracy Score for Model {model_idx_2 + 1}: %.4f", round_and_format(uniform_score_2))

    main_logger.info("\n" + symbol_separator1 * logger_common_symbol_length)
    main_logger.info("Accuracy Scores - Subjective Weights")
    main_logger.info(symbol_separator1 * logger_common_symbol_length)
    main_logger.info(f"Accuracy Score for Model {model_idx_1 + 1}: %.4f", round_and_format(subjective_score_1))
    main_logger.info(f"Accuracy Score for Model {model_idx_2 + 1}: %.4f", round_and_format(subjective_score_2))

    main_logger.info("\n" + symbol_separator1 * logger_common_symbol_length)
    main_logger.info("Accuracy Scores - PCA-Based Weights")
    main_logger.info(symbol_separator1 * logger_common_symbol_length)
    main_logger.info(f"Accuracy Score for Model {model_idx_1 + 1}: %.4f", round_and_format(pca_score_1))
    main_logger.info(f"Accuracy Score for Model {model_idx_2 + 1}: %.4f", round_and_format(pca_score_2))

    main_logger.info("\n" + symbol_separator1 * logger_common_symbol_length)
    main_logger.info("Accuracy Scores - Inverse Variance-Based Weights")
    main_logger.info(symbol_separator1 * logger_common_symbol_length)
    main_logger.info(f"Accuracy Score for Model {model_idx_1 + 1}: %.4f", round_and_format(inverse_variance_score_1))
    main_logger.info(f"Accuracy Score for Model {model_idx_2 + 1}: %.4f", round_and_format(inverse_variance_score_2))

    main_logger.info("\n" + symbol_separator1 * logger_common_symbol_length)
    main_logger.info("Accuracy Scores - AHP-Based Weights")
    main_logger.info(symbol_separator1 * logger_common_symbol_length)
    main_logger.info(f"Accuracy Score for Model {model_idx_1 + 1}: %.4f", round_and_format(ahp_score_1))
    main_logger.info(f"Accuracy Score for Model {model_idx_2 + 1}: %.4f", round_and_format(ahp_score_2))

    main_logger.info("\n" + symbol_separator1 * logger_common_symbol_length)
    main_logger.info("General Accuracy Scores")
    main_logger.info(symbol_separator1 * logger_common_symbol_length)
    main_logger.info(f"General Accuracy Scores for Model {model_idx_1 + 1}: %.4f", round_and_format(average_score_1))
    main_logger.info(f"General Accuracy Scores for Model {model_idx_2 + 1}: %.4f", round_and_format(average_score_2))

    main_logger.info("\n" + symbol_separator2 * logger_common_symbol_length)
    main_logger.info("Better Model from Two-Model Direct Comparison")
    main_logger.info(symbol_separator2 * logger_common_symbol_length)
    if average_score_1 < average_score_2:
        main_logger.info(f"Model {model_idx_1 + 1} has a better General Accuracy Score.")
    else:
        main_logger.info(f"Model {model_idx_2 + 1} has a better General Accuracy Score.")

    main_logger.info(f"Model {model_idx_1 + 1} won {wins_1}/5 methods")
    main_logger.info(f"Model {model_idx_2 + 1} won {wins_2}/5 methods")
    
    if average_score_1 < average_score_2 and wins_1 > wins_2:
        main_logger.info("\n" + formatted_message1)
    elif average_score_2 < average_score_1 and wins_2 > wins_1:
        main_logger.info("\n" + formatted_message2)
    elif (average_score_1 < average_score_2 and wins_2 > wins_1) or (average_score_2 < average_score_1 and wins_1 > wins_2):
        main_logger.info("\n" + formatted_message3)
    else:
        main_logger.info("\n" + formatted_message4)

    main_logger.info("\n" + symbol_separator2 * logger_common_symbol_length)
    main_logger.info("Final Outcome from Multi-Model Round-Robin")
    main_logger.info(symbol_separator2 * logger_common_symbol_length)

    comparison_results = round_robin_comparisons(model_dirs)

    for model_index, scores in comparison_results.items():
        valid_scores = [score for score in scores if not np.isnan(score).any()]
        if valid_scores:
            average_agas = np.mean(valid_scores)
            main_logger.info(f"Model {model_index + 1}: Mean General Accuracy Score = {average_agas:.4f}")

    valid_comparisons = {k: v for k, v in comparison_results.items() if v}

    best_model_index = min(valid_comparisons, key=lambda k: np.mean([score for score in valid_comparisons[k] if not np.isnan(score).any()]))
    main_logger.info("\n" + f"Model {best_model_index + 1} has the best MGAS of: {np.mean([score for score in valid_comparisons[best_model_index] if not np.isnan(score).any()]):.4f}")

    all_stats = log_stream.getvalue()
    with open(output_stats_file, "w") as f:
        f.write(all_stats)


if __name__ == "__main__":
    main()
