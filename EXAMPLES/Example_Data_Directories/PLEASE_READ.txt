Here are some example data to help you to understand, and experiment with, my scripts. These folders represent the results of three important experiments I conducted on a model I'm creating from RealVisXL-Ver4.0:

1. Folders 'data_model_1' and 'data_model_2' correspond to the test of the checkpoint models, using the same training set and hyperparameters, only differing by the _H data using the MIN_SNR_GAMMA loss weight function and _not uses CONSTANT.
-- The filenames were intentional: The set of five tested reference images (i.e. a,b,g,n,z), in three randomly chosen batches (i.e. 1,2,3) formed form an overall set of images (for each model), and checked between the _H and _not checkpoints. You do not have to follow this convention to use the script, just an FYI.

2. The following four folders beginning with 'previews_' signify my tests on model _H after I determined that it had a better chance of replicating the likeness of the trained subject. These four folders represent my tests over four samplers, 'dpm++2M-Karras-25 steps', 'dpmpp3M-SDE-Exponential-30 steps', 'dpmpp-SDE-Karras-20 steps', and 'Restart_Karras_40 steps'. These were chosen based on popularity, what works best with the base model checkpoint, and with step counts I've chosen based on my own experience of what worked best for each sampler.
-- These filenames were NOT intentional. The script will generate these filenames based on the filenames of the used reference images.
-- Apparently, 'previews_dpmpp3MSDE_Exp_30' is the best, but I'm not sure if that's because I didn't hold everything fixed for this "quick" check and used various evaluation steps and a different scheduler. Dunno, needs further testing with more careful control of the variables.

3. The following two folders beginning with 'rand_' represents my "SANITY CHECK" tests -- since it dawned on me, and to my horror after dozens of hours of coding -- that I never bothered to see what my script does when the reference images were run against images of a different subject, i.e. the images are of random people while the fixed reference images are of the same person in different views, i.e. i.e. test a freakin' control group!
-- First folder is random people against fixed set of references; Second folder is synthetic normal distribution (samples: 10000, mean: 0.5, SD: 0.075)
-- These tests went as expected and helped me define a THRESHOLD for checking the script's success. 
-- When testing against random people from my regularization (REG) set, the histogram distribution should be centered very close to a value of 0.5, i.e. have a Mean and Median very close to that (not `exactly` 0.5 always, since the REG data IS baked into the model after all, but nearly so). 
-- When tested with correlated generated images (model trained on the reference subject) the center should be much smaller and closer to the approximate range, [0.2 - 0.3] (may differ based on quality of trained model; I'd say mine is Rated: "Pretty Dang Good" [~0.21]).

>>> HOW TO USE EXAMPLE DIR <<<
>>> Feel free to mix-n-match as many directories as you want to test the scripts capabilities. To do so, move as many of the example directories (folders) as you want to test into the 'output' subdirectory of the 'DIR' folder. The script will analyze as many model data folders as you place in the 'output' folder. 
--> Keep in mind that only the first two folders (by order in directory, name, ascending) will be used for the two-model comparison test, by default. Change the 'models_to_compare_directly' USER DEFINED variable in the GUI in order to compare a different pair of models. For example: use 3,4 to compare the third and fourth models (default is 1,2; compare first and second models).
--> Keep in mind that all models will be used for the Round-Robin tournament style comparison test. This test is only meaningfully different from the direct comparison when you have more than two models worth of dataset folders.
--> The example figures were created using all example directories (except the synthetic normal) at once, and comparing model 1 with 7 (i.e. 'data_model_1' and 'rand_combined'), which it isn't really recommended to mix different experiment types like that (comparison may become meaningless, not sure), but was fun to test the capabilities of the script. Script intended to test similar models to find the best and guide training methodology.
>>>>>>>>>>>><<<<<<<<<<<<<<<<<